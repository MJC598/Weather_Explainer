{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99398b64-6e8f-4d0d-8a5b-57b3d36c8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "from lime import lime_tabular as ltb\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from sklearn.utils import shuffle\n",
    "import sys\n",
    "\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3dd754-75fb-426c-a228-2225dc57191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baselineRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0,\n",
    "                h0=None,\n",
    "                c0=None):\n",
    "        super(baselineRNN, self).__init__()\n",
    "        self.rnn1 = nn.RNN(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = h0\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, h_n  = self.rnn1(x,self.h0)\n",
    "\n",
    "        # take all outputs\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineLSTM(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0,\n",
    "                 h0=None,\n",
    "                 c0=None):\n",
    "        super(baselineLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size=input_size,hidden_size=hidden_size,\n",
    "                           num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = h0\n",
    "        self.c0 = c0\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (h_n, c_n)  = self.rnn(x,(self.h0,self.c0))\n",
    "\n",
    "        # take all outputs\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out\n",
    "\n",
    "class baselineGRU(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size=1,\n",
    "                 batch_size=1,num_layers=1,batch_first=True,dropout=0.0,\n",
    "                h0=None,\n",
    "                c0=None):\n",
    "        super(baselineGRU, self).__init__()\n",
    "        self.rnn = nn.GRU(input_size=input_size,hidden_size=hidden_size,\n",
    "                          num_layers=num_layers,batch_first=batch_first,dropout=dropout)\n",
    "        self.lin = nn.Linear(hidden_size,output_size)\n",
    "        self.h0 = h0\n",
    "        self.sm = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.h0.shape)\n",
    "        x, h_n  = self.rnn(x,self.h0)\n",
    "\n",
    "        # take last cell output\n",
    "        out = self.lin(x[:, -1, :])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "975e0f39-b4a6-4cc3-8cf2-028c5b4f6795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Albury\n",
      "Albury2\n",
      "Albury30\n",
      "Albany29\n",
      "(1444, 1)\n",
      "(1444, 116, 20)\n"
     ]
    }
   ],
   "source": [
    "CSV_FILE = '/home/matt/data/Rain_In_Australia/fragweatherAUS.csv'\n",
    "LOSS_PATH = 'losses/LSTM.csv'\n",
    "MODEL_PATH = 'models/LSTM.pt'\n",
    "df_labels_list = []\n",
    "df_data_list = []\n",
    "df = pd.read_csv(CSV_FILE)\n",
    "list_idx = -1\n",
    "maxval = 0\n",
    "runner = 0\n",
    "for index, row in df.iterrows():\n",
    "    if index == 0 or df.loc[index-1, 'Location'] != row['Location']:\n",
    "        df_labels_list.append(np.array(row['RainTomorrow']))\n",
    "        df_data_list.append(row['MinTemp':'RainToday'].to_numpy())\n",
    "        list_idx += 1\n",
    "        if runner > maxval:\n",
    "            print(df.loc[index-1, 'Location'])\n",
    "            maxval = runner\n",
    "        runner = 1\n",
    "    else:\n",
    "        df_labels_list[list_idx] = np.vstack((df_labels_list[list_idx], np.array(row['RainTomorrow'])))\n",
    "        df_data_list[list_idx] = np.vstack((df_data_list[list_idx], row['MinTemp':'RainToday'].to_numpy()))\n",
    "        runner += 1\n",
    "        \n",
    "mask = np.zeros((maxval, 20))\n",
    "label_mask = np.zeros((maxval, 1))\n",
    "rs_data = []\n",
    "rs_label = []\n",
    "for i in range(len(df_data_list)):\n",
    "    mask = np.zeros((maxval, 20))\n",
    "    label_mask = np.zeros((maxval, 1))\n",
    "    x_offset = mask.shape[0] - df_data_list[i].shape[0]\n",
    "    mask[x_offset:df_data_list[i].shape[0]+x_offset,:]=df_data_list[i]\n",
    "    rs_data.append(mask)\n",
    "    label_mask[x_offset:df_labels_list[i].shape[0]+x_offset,:]=df_labels_list[i]\n",
    "    rs_label.append(label_mask)\n",
    "    \n",
    "labels = np.zeros((len(rs_label), rs_label[0].shape[1]))\n",
    "for i in range(len(rs_data)):\n",
    "    labels[i,:] = rs_label[i][-1,:]\n",
    "    for j in range(20):\n",
    "        rs_data[i][:,j] += 1 + (-1*min(rs_data[i][:,j]))\n",
    "        rs_data[i][:,j] = np.diff(rs_data[i][:,j],n=2,axis=0, append=[-100,-100])\n",
    "#     print(rs_data[i].shape)\n",
    "#     df_labels_list[i] = torch.Tensor(df_labels_list[i].astype('float64'))\n",
    "#     df_data_list[i] = torch.Tensor(df_data_list[i].astype('float64'))\n",
    "data = np.stack(rs_data)\n",
    "# labels = np.stack(rs_label)\n",
    "data, labels = shuffle(data, labels, random_state=0)\n",
    "print(labels.shape)\n",
    "print(data.shape)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eea04860-462f-4ccf-be6e-b3282d483c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,save_filepath,training_loader,validation_loader,epochs,device):\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    epochs_list = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    training_len = len(training_loader.dataset)\n",
    "    validation_len = len(validation_loader.dataset)\n",
    "\n",
    "    #splitting the dataloaders to generalize code\n",
    "    data_loaders = {\"train\": training_loader, \"val\": validation_loader}\n",
    "\n",
    "    \"\"\"\n",
    "    This is your optimizer. It can be changed but Adam is generally used. \n",
    "    Learning rate (alpha in gradient descent) is set to 0.001 but again \n",
    "    can easily be adjusted if you are getting issues\n",
    "\n",
    "    Loss function is set to Mean Squared Error. If you switch to a classifier \n",
    "    I'd recommend switching the loss function to nn.CrossEntropyLoss(), but this \n",
    "    is also something that can be changed if you feel a better loss function would work\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "#     loss_func = nn.MSELoss()\n",
    "#     loss_func = nn.L1Loss()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "#     loss_func = nn.BCELoss()\n",
    "    decay_rate = 0.90 #decay the lr each step to 93% of previous lr\n",
    "    lr_sch = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "    total_start = time.time()\n",
    "\n",
    "    \"\"\"\n",
    "    You can easily adjust the number of epochs trained here by changing the number in the range\n",
    "    \"\"\"\n",
    "    for epoch in tqdm(range(epochs), position=0, leave=True):\n",
    "        start = time.time()\n",
    "        train_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        temp_loss = 100000000000000.0\n",
    "        correct = 0\n",
    "        train_correct = 0\n",
    "        val_correct = 0\n",
    "        train_total = 0\n",
    "        val_total = 0\n",
    "        total = 0\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)\n",
    "            else:\n",
    "                model.train(False)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for i, (x, y) in enumerate(data_loaders[phase]):  \n",
    "                x = x.to(device)\n",
    "                y = torch.squeeze(y,1).type(torch.LongTensor).to(device)\n",
    "                output = model(x)\n",
    "                loss = loss_func(output.type(torch.FloatTensor).to(device), y)\n",
    "                out_max = torch.max(output, 1)[1]\n",
    "                correct += (out_max == y).detach().cpu().float().sum()\n",
    "                total += list(y.size())[0]\n",
    "        \n",
    "                #backprop             \n",
    "                optimizer.zero_grad()           \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    train_correct += (out_max == y).detach().cpu().float().sum()\n",
    "                    train_total += list(y.size())[0]\n",
    "                else:\n",
    "                    val_correct += (out_max == y).detach().cpu().float().sum()\n",
    "                    val_total += list(y.size())[0]\n",
    "\n",
    "                #calculating total loss\n",
    "                running_loss += loss.item()\n",
    "#                 print(loss.item())\n",
    "            \n",
    "            if phase == 'train':\n",
    "                train_loss = running_loss\n",
    "                lr_sch.step()\n",
    "            else:\n",
    "                val_loss = running_loss\n",
    "\n",
    "        end = time.time()\n",
    "        # shows total loss\n",
    "        if epoch%5 == 0:\n",
    "            tqdm.write('Train Accuracy: {} Val Accuracy: {}'.format(train_correct/train_total, val_correct/val_total))\n",
    "            tqdm.write('[%d, %5d] train loss: %.6f val loss: %.6f' % (epoch + 1, i + 1, train_loss, val_loss))\n",
    "#         print(end - start)\n",
    "        \n",
    "        #saving best model\n",
    "        if val_loss < temp_loss:\n",
    "            torch.save(model, save_filepath)\n",
    "            temp_loss = val_loss\n",
    "        epochs_list.append(epoch)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append((train_correct/train_total)*100)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append((val_correct/val_total)*100)\n",
    "    total_end = time.time()\n",
    "#     print(total_end - total_start)\n",
    "    #Creating loss csv\n",
    "    loss_df = pd.DataFrame(\n",
    "        {\n",
    "            'epoch': epochs_list,\n",
    "            'training loss': train_loss_list,\n",
    "            'validation loss': val_loss_list,\n",
    "            'training accuracy': train_acc_list,\n",
    "            'validation accuracy': val_acc_list\n",
    "        }\n",
    "    )\n",
    "    # Writing loss csv, change path to whatever you want to name it\n",
    "    loss_df.to_csv(LOSS_PATH, index=None)\n",
    "    return train_loss_list, val_loss_list, train_acc_list, val_acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08061588-03ef-4817-8fac-9aadb5ae23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SeqDataset(torch.utils.data.dataset.Dataset):\n",
    "#     def __init__(self, _dataset, _labels):\n",
    "#         self.dataset = _dataset\n",
    "#         self.labels = _labels\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         example = self.dataset[index]\n",
    "#         target = self.labels[index]\n",
    "#         return np.array(example), target\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset)\n",
    "    \n",
    "# train_loader = torch.utils.data.DataLoader(dataset=SeqDataset(df_data_list[:40], df_labels_list[:40]),\n",
    "#                                            batch_size=1,\n",
    "#                                            shuffle=False)\n",
    "\n",
    "# validation_loader = torch.utils.data.DataLoader(dataset=SeqDataset(df_data_list[40:], df_labels_list[40:]),\n",
    "#                                            batch_size=1,\n",
    "#                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5754c66-719d-4a69-8568-f7083c7d144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "k = 1150\n",
    "train_loader = DataLoader(dataset=TensorDataset(torch.Tensor(data[:k,:,:]), torch.Tensor(labels[:k,:])),batch_size=batch_size,shuffle=True)\n",
    "validation_loader = DataLoader(dataset=TensorDataset(torch.Tensor(data[k:,:,:]), torch.Tensor(labels[k:,:])),batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f51e7-000e-49c0-a7b2-d5401d82c3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eee16cef303423098fdaa03000e4127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[1,   294] train loss: 639.013914 val loss: 149.288419\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[6,   294] train loss: 571.539342 val loss: 145.920024\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8061224222183228\n",
      "[11,   294] train loss: 568.371408 val loss: 145.684501\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8061224222183228\n",
      "[16,   294] train loss: 566.218399 val loss: 145.626372\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[21,   294] train loss: 564.940058 val loss: 145.645571\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[26,   294] train loss: 564.228825 val loss: 145.679095\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[31,   294] train loss: 563.830462 val loss: 145.734223\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[36,   294] train loss: 563.604418 val loss: 145.715057\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[41,   294] train loss: 563.441598 val loss: 145.717230\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[46,   294] train loss: 563.350862 val loss: 145.716852\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[51,   294] train loss: 563.298089 val loss: 145.716561\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[56,   294] train loss: 563.266069 val loss: 145.715849\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[61,   294] train loss: 563.247128 val loss: 145.716161\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[66,   294] train loss: 563.237065 val loss: 145.716561\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[71,   294] train loss: 563.230205 val loss: 145.716117\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[76,   294] train loss: 563.226239 val loss: 145.715680\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[81,   294] train loss: 563.223871 val loss: 145.715218\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[86,   294] train loss: 563.222422 val loss: 145.714747\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[91,   294] train loss: 563.221644 val loss: 145.714256\n",
      "Train Accuracy: 0.7991304397583008 Val Accuracy: 0.8027210831642151\n",
      "[96,   294] train loss: 563.221270 val loss: 145.713909\n"
     ]
    }
   ],
   "source": [
    "input_size = 20\n",
    "hidden_size = 35\n",
    "output_size = 2\n",
    "num_layers = 1\n",
    "batch_first = True\n",
    "dropout = 0.0\n",
    "epochs = 100\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "h0 = torch.randn(num_layers, batch_size, hidden_size).to(device)\n",
    "c0 = torch.randn(num_layers, batch_size, hidden_size).to(device)\n",
    "model = baselineLSTM(input_size, hidden_size, output_size, batch_size, num_layers, batch_first, dropout, h0,c0)\n",
    "\n",
    "train_loss, validation_loss, train_acc, val_acc = train_model(model,MODEL_PATH,train_loader,validation_loader,epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8875bd-f956-4e79-b5b2-69e3cb7d08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2)\n",
    "fig.tight_layout()\n",
    "ax[0,0].plot(range(epochs), validation_loss)\n",
    "ax[0,0].set_title('Validation Loss')\n",
    "ax[0,0].set_ylabel('Loss')\n",
    "ax[0,0].set_xlabel('Epoch')\n",
    "\n",
    "ax[0,1].plot(range(epochs), train_loss)\n",
    "ax[0,1].set_title('Training Loss')\n",
    "ax[0,1].set_ylabel('Loss')\n",
    "ax[0,1].set_xlabel('Epoch')\n",
    "\n",
    "ax[1,0].plot(range(epochs), val_acc)\n",
    "ax[1,0].set_title('Validation Accuracy')\n",
    "ax[1,0].set_ylabel('Accuracy')\n",
    "ax[1,0].set_ylim((0,100))\n",
    "ax[1,0].set_xlabel('Epoch')\n",
    "\n",
    "ax[1,1].plot(range(epochs), train_acc)\n",
    "ax[1,1].set_title('Training Accuracy')\n",
    "ax[1,1].set_ylabel('Accuracy')\n",
    "ax[1,1].set_ylim((0,100))\n",
    "ax[1,1].set_xlabel('Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991563e-62ab-4b92-8b50-e7679d0fa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(arr):\n",
    "    model.eval()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    preds = np.zeros((arr.shape[0], output_size))\n",
    "    print(arr.shape)\n",
    "    for i in range(arr.shape[0]):\n",
    "#         arr[i,:,:] = torch.Tensor(arr[i,:,:]).to(device)\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        pred = sm(model(torch.Tensor(np.reshape(arr[i,:,:], (1,arr.shape[1],arr.shape[2]))).to(device)))\n",
    "        preds[i,:] = pred.detach().cpu().numpy()\n",
    "        \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e4bce-8674-485b-a890-4f3caec16677",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names =['MinTemp', 'MaxTemp', 'Rain', 'Evap', 'Sun', 'GustDir',\n",
    "             'GustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSp9am', 'WindSp3pm',\n",
    "             'Hum9am', 'Hum3pm', 'Pres9am', 'Pres3pm', 'Cloud9am', \n",
    "             'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday']\n",
    "class_names = ['No Rain', 'Rain']\n",
    "exp = ltb.RecurrentTabularExplainer(data,training_labels=labels,feature_names=feat_names,class_names=class_names)\n",
    "sample = 4\n",
    "explanation = exp.explain_instance(data[sample,:,:],predict_fn,num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf983363-a22f-4e7d-a11e-ea925fe95f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "explanation.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e802e-f1db-451e-a69d-4aafc19d5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(explanation.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cb4f9-d739-4373-a7b8-0de9ffcb3203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
